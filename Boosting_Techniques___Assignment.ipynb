{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Boosting Techniques | Assignment**"
      ],
      "metadata": {
        "id": "SOKephNRhTo-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Boosting in Machine Learning? Explain how it improves weak\n",
        "learners.\n",
        "- Boosting is an ensemble learning technique that combines multiple weak learners (models that perform slightly better than random guessing) to build a strong and highly accurate model. Instead of training all models independently (like bagging), boosting trains them sequentially, where each new model focuses on correcting the errors made by the previous one.\n",
        "\n",
        "Question 2: What is the difference between AdaBoost and Gradient Boosting in terms\n",
        "of how models are trained?\n",
        "- AdaBoost (Adaptive Boosting)\n",
        "\n",
        "  AdaBoost trains weak learners sequentially, focusing on misclassified samples.\n",
        "\n",
        "  It adjusts sample weights after each iteration:\n",
        "\n",
        "  Misclassified samples → weight increases\n",
        "\n",
        "  Correctly classified samples → weight decreases\n",
        "\n",
        "  The next learner is trained on this reweighted dataset.\n",
        "\n",
        "  It combines learners using error-based weights.\n",
        "\n",
        "- Gradient Boosting\n",
        "\n",
        "  Gradient Boosting also trains models sequentially, but instead of adjusting sample weights, it fits each new learner to the residual errors (gradients) of the previous model.\n",
        "\n",
        "  It uses gradient descent to minimize a differentiable loss function (e.g., MSE, Log-loss).\n",
        "\n",
        "  Each new weak learner tries to reduce the loss step-by-step.\n",
        "\n",
        "Question 3: How does regularization help in XGBoost?\n",
        "- Regularization in XGBoost is one of the key reasons it performs better than traditional Gradient Boosting. It helps control model complexity and prevents overfitting.\n",
        "\n",
        "Question 4: Why is CatBoost considered efficient for handling categorical data?\n",
        "- CatBoost is considered highly efficient for handling categorical data because it processes categorical features internally and automatically, without requiring manual encoding techniques such as one-hot encoding or label encoding.\n",
        "\n",
        "Question 5: What are some real-world applications where boosting techniques are\n",
        "preferred over bagging methods?\n",
        "\n",
        "- Boosting techniques are preferred over bagging when the problem requires high accuracy, handling complex patterns, and reducing bias more effectively than variance. Because boosting focuses on correcting previous errors, it performs exceptionally well in several real-world scenarios.\n",
        "\n"
      ],
      "metadata": {
        "id": "HwVKBiIBhTel"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to:\n",
        "● Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "● Print the model accuracy"
      ],
      "metadata": {
        "id": "xXmcCtZojYli"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pXwcAKHhR8F",
        "outputId": "d141d9fb-402b-4269-d16f-c3cf371c6c8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Classifier Accuracy: 0.9736842105263158\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into Train and Test Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize AdaBoost Classifier\n",
        "model = AdaBoostClassifier(n_estimators=100, learning_rate=1.0, random_state=42)\n",
        "\n",
        "# Train the Model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"AdaBoost Classifier Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "● Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "● Evaluate performance using R-squared score\n"
      ],
      "metadata": {
        "id": "rSTxLkjnjcHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load California Housing Dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize Model\n",
        "model = GradientBoostingRegressor(\n",
        "    n_estimators=200,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train Model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate R² Score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Gradient Boosting Regressor R-squared Score:\", r2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdVZJ_Q9jfeA",
        "outputId": "a902d1b4-4f60-482f-9bf9-7bf8225bc3fc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Regressor R-squared Score: 0.8004451261281281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "● Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "● Tune the learning rate using GridSearchCV\n",
        "● Print the best parameters and accuracy\n"
      ],
      "metadata": {
        "id": "QIf7qjMwjjqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# XGBoost Classifier\n",
        "model = XGBClassifier(\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Hyperparameter Grid (tuning only learning rate)\n",
        "param_grid = {\n",
        "    'learning_rate': [0.001, 0.01, 0.05, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "# GridSearchCV\n",
        "grid = GridSearchCV(\n",
        "    estimator=model,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Train Model\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Best Parameters\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "\n",
        "# Predict using the best model\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"XGBoost Classifier Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krx4s9kWjncy",
        "outputId": "c5d5d78a-ace2-4392-b01a-8af757c5b6a4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [12:54:11] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'learning_rate': 0.2}\n",
            "XGBoost Classifier Accuracy: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "● Train a CatBoost Classifier\n",
        "● Plot the confusion matrix using seaborn"
      ],
      "metadata": {
        "id": "fzPOGrt2jqU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train CatBoost Classifier\n",
        "model = CatBoostClassifier(iterations=200, learning_rate=0.1, verbose=0)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot matrix\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\")\n",
        "plt.title(\"Confusion Matrix - CatBoost Classifier\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gsVtZy6Glda1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You're working for a FinTech company trying to predict loan default using\n",
        "customer demographics and transaction behavior.\n",
        "The dataset is imbalanced, contains missing values, and has both numeric and\n",
        "categorical features.\n",
        "Describe your step-by-step data science pipeline using boosting techniques:\n",
        "● Data preprocessing & handling missing/categorical values\n",
        "● Choice between AdaBoost, XGBoost, or CatBoost\n",
        "● Hyperparameter tuning strategy\n",
        "● Evaluation metrics you'd choose and why\n",
        "● How the business would benefit from your model\n",
        "\n",
        "\n",
        "-"
      ],
      "metadata": {
        "id": "poI53SuHkmiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier, Pool\n",
        "# preprocess: impute medians, fill categorical missing with \"<MISSING>\", engineer features\n",
        "cat_features = [index list]\n",
        "\n",
        "model = CatBoostClassifier(\n",
        "    iterations=2000,\n",
        "    learning_rate=0.03,\n",
        "    depth=6,\n",
        "    l2_leaf_reg=3,\n",
        "    eval_metric='AUC',\n",
        "    random_seed=42,\n",
        "    early_stopping_rounds=100,\n",
        "    auto_class_weights='Balanced'\n",
        ")\n",
        "\n",
        "train_pool = Pool(X_train, y_train, cat_features=cat_features)\n",
        "val_pool   = Pool(X_val, y_val, cat_features=cat_features)\n",
        "\n",
        "model.fit(train_pool, eval_set=val_pool)\n",
        "probs = model.predict_proba(X_test)[:, 1]\n",
        "# evaluate: average_precision_score(y_test, probs) ; compute business cost for thresholds\n"
      ],
      "metadata": {
        "id": "FwQAPwykl_aN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}