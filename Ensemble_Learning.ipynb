{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Ensemble Learning**"
      ],
      "metadata": {
        "id": "7HLzmnBtbqOj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 Can we use Bagging for regression problems\n",
        "- Bagging (Bootstrap Aggregating) is a general ensemble technique that reduces variance by training multiple models on different bootstrap samples of the data. It works for both:\n",
        "\n",
        " - Classification\n",
        "\n",
        "- Regression\n",
        "\n",
        "2 What is the difference between multiple model training and single model training\n",
        "- 1️⃣ Single Model Training:-\n",
        "Single model training refers to building one machine learning model on the entire dataset.\n",
        "The model learns all patterns and makes predictions alone.\n",
        "\n",
        "- 2️⃣ Multiple Model Training:-\n",
        "Multiple model training refers to building several models on the dataset—either on different samples, different features, or different algorithms—and combining their predictions.\n",
        "\n",
        "3 Explain the concept of feature randomness in Random Forest?\n",
        "- Feature randomness in a Random Forest means that each decision tree is allowed to consider only a random subset of features when splitting a node.\n",
        "\n",
        "4  What is OOB (Out-of-Bag) Score?\n",
        "- OOB (Out-of-Bag) Score is a built-in validation score used in Bagging and Random Forest models.\n",
        "It is calculated using the data samples not included in the bootstrap sample for each tree.\n",
        "\n",
        "5 How can you measure the importance of features in a Random Forest model\n",
        "- Gini Importance (MDI): Based on total impurity reduction contributed by each feature across all trees.\n",
        "\n",
        "- Permutation Importance (MDA): Measures drop in model accuracy when a feature is randomly permuted.\n",
        "\n",
        "6  Explain the working principle of a Bagging Classifier\n",
        "- A Bagging Classifier (Bootstrap Aggregating) improves model performance by training multiple versions of the same model on different random samples of the data and combining their predictions.\n",
        "\n",
        "7  How do you evaluate a Bagging Classifier’s performance\n",
        "- A Bagging Classifier is evaluated using the same performance metrics as other classification models.\n",
        "The goal is to measure how well the ensemble predicts unseen data.\n",
        "\n",
        "8 How does a Bagging Regressor work\n",
        "- A Bagging Regressor uses the Bagging (Bootstrap Aggregating) technique to improve prediction accuracy and reduce variance in regression tasks.\n",
        "\n",
        "9 What is the main advantage of ensemble techniques\n",
        "- The main advantage of ensemble techniques is that they combine multiple models to achieve higher accuracy, better stability, and improved generalization compared to individual models.\n",
        "\n",
        "10 What is the main challenge of ensemble methods\n",
        "- The main challenge of ensemble methods is their increased complexity, which makes them computationally expensive, harder to interpret, and more difficult to train and deploy compared to single models.\n",
        "\n",
        "11 Explain the key idea behind ensemble technique\n",
        "- The key idea behind ensemble techniques is to combine multiple models to produce a stronger, more accurate, and more stable final prediction than any single model could achieve\n",
        "\n",
        "12 What is a Random Forest Classifier\n",
        "- A Random Forest Classifier is an ensemble learning algorithm that builds multiple decision trees and combines their predictions using majority voting to perform classification tasks.\n",
        "\n",
        "13 What are the main types of ensemble techniques\n",
        "- The main types of ensemble techniques are Bagging (to reduce variance), Boosting (to reduce bias), and Stacking (to combine diverse models using a meta-learner).\n",
        "\n",
        "14 What is ensemble learning in machine learning\n",
        "- Ensemble learning is a technique in machine learning where multiple models are combined to make a final prediction that is more accurate, stable, and reliable than any single model.\n",
        "\n",
        "15 When should we avoid using ensemble methods\n",
        "- Avoid ensemble methods when interpretability is required, when computational resources are limited, when the dataset is very small, or when a simple model already performs well.\n",
        "\n",
        "16 How does Bagging help in reducing overfitting\n",
        "- Bagging (Bootstrap Aggregating) reduces overfitting by training multiple models on different random samples of the data and averaging their predictions, which smooths out noise and reduces variance.\n",
        "\n",
        "17 Why is Random Forest better than a single Decision Tree\n",
        "- A Random Forest is better because it combines many decision trees to create a more accurate, stable, and generalizable model, while a single decision tree is prone to overfitting and instability.\n",
        "\n",
        "18 What is the role of bootstrap sampling in Bagging\n",
        "- Bootstrap sampling is the core mechanism that creates diversity among the models in Bagging.\n",
        "It involves drawing random samples with replacement from the training data to build multiple different datasets for training.\n",
        "\n",
        "19 What are some real-world applications of ensemble techniques\n",
        "- Ensemble methods are widely used because they improve accuracy, reduce errors, and provide stable results. Here are some common applications:\n",
        "\n",
        "20 What is the difference between Bagging and Boosting\n",
        "- Bagging (Bootstrap Aggregating) — Bagging is an ensemble technique where multiple models (usually of the same type) are trained independently on different bootstrapped samples of the training data, and their predictions are averaged (regression) or voted (classification) to reduce variance and prevent overfitting.\n",
        "\n",
        "- Boosting — Boosting is an ensemble technique where multiple weak models are trained sequentially, and each new model focuses on correcting the errors made by the previous ones. Boosting reduces bias and creates a strong final model by giving more weight to difficult samples."
      ],
      "metadata": {
        "id": "PRzZWjN9bqtj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy"
      ],
      "metadata": {
        "id": "qEJT0y0Hfe-J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XuH3pT8Yaed7"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load sample dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into train & test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create Bagging Classifier with Decision Trees as base estimator\n",
        "bagging_model = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "bagging_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = bagging_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)"
      ],
      "metadata": {
        "id": "0-pNTXOEfnWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load sample regression dataset\n",
        "data = load_boston()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create Bagging Regressor using Decision Trees\n",
        "bagging_regressor = BaggingRegressor(\n",
        "    base_estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "bagging_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = bagging_regressor.predict(X_test)\n",
        "\n",
        "# Calculate MSE\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n"
      ],
      "metadata": {
        "id": "GTSxJS9vfrp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores"
      ],
      "metadata": {
        "id": "2ylittKkfupQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Random Forest model\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "# Put into a DataFrame for clean output\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': data.feature_names,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print Feature Importance\n",
        "print(feature_importance_df)\n"
      ],
      "metadata": {
        "id": "Yca6XDHafyh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a Random Forest Regressor and compare its performance with a single Decision Tree2"
      ],
      "metadata": {
        "id": "3lnwjG2Rf2Og"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "data = load_boston()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Train a Single Decision Tree\n",
        "# -----------------------------\n",
        "dt = DecisionTreeRegressor(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_pred = dt.predict(X_test)\n",
        "dt_mse = mean_squared_error(y_test, dt_pred)\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Train a Random Forest Regressor\n",
        "# -----------------------------\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "rf.fit(X_train, y_train)\n",
        "rf_pred = rf.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "\n",
        "# -----------------------------\n",
        "# Print results\n",
        "# -----------------------------\n",
        "print(\"Decision Tree MSE:\", dt_mse)\n",
        "print(\"Random Forest MSE:\", rf_mse)\n"
      ],
      "metadata": {
        "id": "8rEbnPbyf1ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier"
      ],
      "metadata": {
        "id": "2nWZ3tBvf98l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split (not required for OOB but used for reference)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create Random Forest with OOB enabled\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    oob_score=True,         # Enable Out-of-Bag evaluation\n",
        "    bootstrap=True,         # Required for OOB\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train model\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Print OOB Score\n",
        "print(\"OOB Score:\", rf.oob_score_)\n"
      ],
      "metadata": {
        "id": "CMjxuDVogB17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a Bagging Classifier using SVM as a base estimator and print accuracy"
      ],
      "metadata": {
        "id": "XfPLj3rpgElZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train–test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create Bagging Classifier with SVM base estimator\n",
        "bagging_svm = BaggingClassifier(\n",
        "    base_estimator=SVC(),\n",
        "    n_estimators=20,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "bagging_svm.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = bagging_svm.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Bagging Classifier Accuracy (SVM Base Estimator):\", accuracy)\n"
      ],
      "metadata": {
        "id": "5hEOSXp8gIB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a Random Forest Classifier with different numbers of trees and compare accuracy"
      ],
      "metadata": {
        "id": "KCFggG9qgKxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train–test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# List of different number of trees\n",
        "n_trees = [10, 50, 100, 200, 300]\n",
        "\n",
        "# Compare accuracy\n",
        "print(\"Number of Trees  |  Accuracy\")\n",
        "print(\"-----------------------------\")\n",
        "\n",
        "for n in n_trees:\n",
        "    rf = RandomForestClassifier(\n",
        "        n_estimators=n,\n",
        "        random_state=42\n",
        "    )\n",
        "    rf.fit(X_train, y_train)\n",
        "    y_pred = rf.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    print(f\"{n:<17} |  {acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "O2Cy_750gOir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score"
      ],
      "metadata": {
        "id": "7RaQh-1SgRJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load binary classification dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train–test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create Bagging Classifier with Logistic Regression\n",
        "bagging_logreg = BaggingClassifier(\n",
        "    estimator=LogisticRegression(max_iter=1000), # Increased max_iter to address convergence warning\n",
        "    n_estimators=20,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "bagging_logreg.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_prob = bagging_logreg.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# AUC Score\n",
        "auc = roc_auc_score(y_test, y_prob)\n",
        "print(\"Bagging Classifier AUC Score (Logistic Regression):\", auc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srY3P7nWgV0Z",
        "outputId": "9c385a62-6829-47b7-b7d7-80fdd8485f9f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier AUC Score (Logistic Regression): 0.9980347199475925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a Random Forest Regressor and analyze feature importance scores"
      ],
      "metadata": {
        "id": "5QZfMwMUgoG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing # Changed load_boston to fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing() # Changed load_boston to fetch_california_housing\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Random Forest Regressor\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "# Convert to DataFrame for clear display\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': data.feature_names,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print Feature Importance\n",
        "print(feature_importance_df)\n"
      ],
      "metadata": {
        "id": "sIoiHiXpgsHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train an ensemble model using both Bagging and Random Forest and compare accuracy."
      ],
      "metadata": {
        "id": "TOIU1rHqgrwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# ------------------------------\n",
        "# 1. Bagging Classifier\n",
        "# ------------------------------\n",
        "bagging_model = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(), # Changed base_estimator to estimator\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bagging_model.fit(X_train, y_train)\n",
        "y_pred_bag = bagging_model.predict(X_test)\n",
        "bagging_acc = accuracy_score(y_test, y_pred_bag)\n",
        "\n",
        "# ------------------------------\n",
        "# 2. Random Forest Classifier\n",
        "# ------------------------------\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "rf_acc = accuracy_score(y_test, y_pred_rf)\n",
        "\n",
        "# ------------------------------\n",
        "# Print results\n",
        "# ------------------------------\n",
        "print(\"Bagging Classifier Accuracy:\", bagging_acc)\n",
        "print(\"Random Forest Accuracy:\", rf_acc)"
      ],
      "metadata": {
        "id": "h84GjK3xgy29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a Random Forest Classifier and tune hyperparameters using GridSearchCV"
      ],
      "metadata": {
        "id": "RLiNzpmjg1Wk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Define parameter grid for tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 5, 10],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Initialize Random Forest model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    n_jobs=-1,           # Use all CPU cores\n",
        "    scoring='accuracy'\n",
        ")\n",
        "\n",
        "# Fit GridSearch\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Hyperparameters:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# Use best model for prediction\n",
        "best_rf = grid_search.best_estimator_\n",
        "y_pred = best_rf.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy after Hyperparameter Tuning:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bikzJCIg_E3",
        "outputId": "878e46f0-56ad-40ff-ba60-c612f7b39c43"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters:\n",
            "{'bootstrap': True, 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "Accuracy after Hyperparameter Tuning: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a Bagging Regressor with different numbers of base estimators and compare performance"
      ],
      "metadata": {
        "id": "s6be3RiyhFqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "data = load_boston()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Different number of estimators to test\n",
        "n_estimators_list = [10, 20, 50, 100, 200]\n",
        "\n",
        "print(\"Estimators  |  MSE\")\n",
        "print(\"-----------------------\")\n",
        "\n",
        "for n in n_estimators_list:\n",
        "    model = BaggingRegressor(\n",
        "        base_estimator=DecisionTreeRegressor(),\n",
        "        n_estimators=n,\n",
        "        random_state=42\n",
        "    )\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "    print(f\"{n:<11} |  {mse:.4f}\")\n"
      ],
      "metadata": {
        "id": "xwwhmDPBhKmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a Random Forest Classifier and analyze misclassified samples"
      ],
      "metadata": {
        "id": "PtZhflenhXyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Find misclassified samples\n",
        "misclassified_indices = (y_test != y_pred)\n",
        "\n",
        "# Create DataFrame for analysis\n",
        "misclassified_df = pd.DataFrame(\n",
        "    X_test[misclassified_indices],\n",
        "    columns=feature_names\n",
        ")\n",
        "misclassified_df[\"Actual\"] = y_test[misclassified_indices]\n",
        "misclassified_df[\"Predicted\"] = y_pred[misclassified_indices]\n",
        "\n",
        "print(\"\\nMisclassified Samples:\")\n",
        "print(misclassified_df)\n"
      ],
      "metadata": {
        "id": "9OcJKSxAhZWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier"
      ],
      "metadata": {
        "id": "KXfyzAhDhmsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train–test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# ----------------------------------\n",
        "# 1. Single Decision Tree Classifier\n",
        "# ----------------------------------\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_pred = dt.predict(X_test)\n",
        "dt_acc = accuracy_score(y_test, dt_pred)\n",
        "\n",
        "# ----------------------------------\n",
        "# 2. Bagging Classifier (with Decision Trees)\n",
        "# ----------------------------------\n",
        "bagging = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=30,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bagging.fit(X_train, y_train)\n",
        "bag_pred = bagging.predict(X_test)\n",
        "bag_acc = accuracy_score(y_test, bag_pred)\n",
        "\n",
        "# ----------------------------------\n",
        "# Print Comparison\n",
        "# ----------------------------------\n",
        "print(\"Single Decision Tree Accuracy:\", dt_acc)\n",
        "print(\"Bagging Classifier Accuracy:\", bag_acc)\n"
      ],
      "metadata": {
        "id": "7aZGoJm7hqS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a Random Forest Classifier and visualize the confusion matrix"
      ],
      "metadata": {
        "id": "GxIR0-Eths5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train–test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=data.target_names)\n",
        "disp.plot(cmap=\"Blues\")\n",
        "plt.title(\"Random Forest Classifier - Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Print accuracy\n",
        "accuracy = rf.score(X_test, y_test)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "_bpK7hd-hsoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy"
      ],
      "metadata": {
        "id": "052IJgLtiDSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Base models\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "svm = SVC(probability=True, random_state=42)\n",
        "lr = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Stacking Classifier\n",
        "estimators = [\n",
        "    (\"decision_tree\", dt),\n",
        "    (\"svm\", svm)\n",
        "]\n",
        "\n",
        "stack_model = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=lr\n",
        ")\n",
        "\n",
        "# Train models\n",
        "dt.fit(X_train, y_train)\n",
        "svm.fit(X_train, y_train)\n",
        "stack_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "dt_pred = dt.predict(X_test)\n",
        "svm_pred = svm.predict(X_test)\n",
        "stack_pred = stack_model.predict(X_test)\n",
        "\n",
        "# Accuracy comparison\n",
        "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, dt_pred))\n",
        "print(\"SVM Accuracy:\", accuracy_score(y_test, svm_pred))\n",
        "print(\"Stacking Classifier Accuracy:\", accuracy_score(y_test, stack_pred))\n"
      ],
      "metadata": {
        "id": "TBDXeo8biIe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a Random Forest Classifier and print the top 5 most important features"
      ],
      "metadata": {
        "id": "fzCespJAiLce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Get feature importances\n",
        "importances = rf.feature_importances_\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    \"Feature\": feature_names,\n",
        "    \"Importance\": importances\n",
        "})\n",
        "\n",
        "# Sort and print top 5\n",
        "top_5 = feature_importance_df.sort_values(by=\"Importance\", ascending=False).head(5)\n",
        "\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(top_5)\n"
      ],
      "metadata": {
        "id": "NmduaTsEiRY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score"
      ],
      "metadata": {
        "id": "2fkYtekEiSsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Bagging Classifier with Decision Tree as base estimator\n",
        "bagging = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train model\n",
        "bagging.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = bagging.predict(X_test)\n",
        "\n",
        "# Evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Bagging Classifier Performance:\")\n",
        "print(\"--------------------------------\")\n",
        "print(f\"Accuracy  : {accuracy:.4f}\")\n",
        "print(f\"Precision : {precision:.4f}\")\n",
        "print(f\"Recall    : {recall:.4f}\")\n",
        "print(f\"F1-Score  : {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "dsSzmPN7iV2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a Random Forest Classifier and analyze the effect of max_depth on accuracy"
      ],
      "metadata": {
        "id": "qO-ESTuXiZp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Different max_depth values to test\n",
        "depth_values = [None, 2, 4, 6, 8, 10, 15, 20]\n",
        "\n",
        "print(\"Effect of max_depth on Random Forest Accuracy\")\n",
        "print(\"---------------------------------------------\")\n",
        "\n",
        "for depth in depth_values:\n",
        "    # Create Random Forest with given max_depth\n",
        "    rf = RandomForestClassifier(\n",
        "        n_estimators=200,\n",
        "        max_depth=depth,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    rf.fit(X_train, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred = rf.predict(X_test)\n",
        "\n",
        "    # Accuracy\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"max_depth = {depth} --> Accuracy = {acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "qxxgepbnic6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare\n",
        "performance"
      ],
      "metadata": {
        "id": "t0V9-Enlih8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load a regression dataset\n",
        "data = load_diabetes()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Bagging with Decision Tree\n",
        "# -------------------------------\n",
        "bag_dt = BaggingRegressor(\n",
        "    base_estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bag_dt.fit(X_train, y_train)\n",
        "y_pred_dt = bag_dt.predict(X_test)\n",
        "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Bagging with KNN\n",
        "# -------------------------------\n",
        "bag_knn = BaggingRegressor(\n",
        "    base_estimator=KNeighborsRegressor(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bag_knn.fit(X_train, y_train)\n",
        "y_pred_knn = bag_knn.predict(X_test)\n",
        "mse_knn = mean_squared_error(y_test, y_pred_knn)\n",
        "\n",
        "# -------------------------------\n",
        "# Compare Performance\n",
        "# -------------------------------\n",
        "print(\"Bagging Regressor Performance Comparison\")\n",
        "print(\"---------------------------------------\")\n",
        "print(f\"Decision Tree + Bagging  | MSE = {mse_dt:.4f}\")\n",
        "print(f\"KNN + Bagging            | MSE = {mse_knn:.4f}\")\n"
      ],
      "metadata": {
        "id": "ZfNlGcS9imo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score"
      ],
      "metadata": {
        "id": "Fo5LJeVlipov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Random Forest Classifier\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train model\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for ROC-AUC\n",
        "y_prob = rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute ROC-AUC Score\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "print(\"Random Forest Classifier Performance:\")\n",
        "print(\"-------------------------------------\")\n",
        "print(f\"ROC-AUC Score : {roc_auc:.4f}\")\n"
      ],
      "metadata": {
        "id": "NPHHQQ0riuEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a Bagging Classifier and evaluate its performance using cross-validatio."
      ],
      "metadata": {
        "id": "eh-Se-Diiwyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Bagging Classifier with Decision Tree base estimator\n",
        "bagging = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Perform 5-Fold Cross-Validation\n",
        "cv_scores = cross_val_score(bagging, X, y, cv=5, scoring='accuracy')\n",
        "\n",
        "# Print results\n",
        "print(\"Bagging Classifier Cross-Validation Performance\")\n",
        "print(\"----------------------------------------------\")\n",
        "print(f\"Cross-Validation Scores : {cv_scores}\")\n",
        "print(f\"Mean Accuracy           : {cv_scores.mean():.4f}\")\n",
        "print(f\"Standard Deviation      : {cv_scores.std():.4f}\")\n"
      ],
      "metadata": {
        "id": "K4sMA8I7i0iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a Random Forest Classifier and plot the Precision-Recall curv"
      ],
      "metadata": {
        "id": "CnCkDeCdi36L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import auc\n",
        "\n",
        "# 1. Generate Synthetic Data\n",
        "# Create a binary classification dataset for demonstration\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=0,\n",
        "                           n_classes=2, n_clusters_per_class=1, random_state=42)\n",
        "\n",
        "# 2. Split Data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Train Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# 4. Calculate Probabilities (for the positive class, index 1)\n",
        "# Precision-Recall curve requires probability scores\n",
        "y_scores = rf_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# 5. Calculate Precision-Recall Curve Points and AUC\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
        "pr_auc = auc(recall, precision)\n",
        "\n",
        "# 6. Plot the Curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "# Plot the curve with the calculated PR-AUC value in the label\n",
        "plt.plot(recall, precision, label=f'Random Forest (PR-AUC = {pr_auc:.4f})', color='darkorange')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve for Random Forest Classifier')\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.grid(True)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.savefig('precision_recall_curve.png')"
      ],
      "metadata": {
        "id": "V13NiS4Ui8Jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy"
      ],
      "metadata": {
        "id": "S_R-VhMHjoMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Generate Synthetic Data\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=0,\n",
        "                           n_classes=2, n_clusters_per_class=1, random_state=42)\n",
        "\n",
        "# 2. Split Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Instantiate Base Estimators\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "lr = LogisticRegression(random_state=42, solver='liblinear')\n",
        "\n",
        "# 4. Instantiate Stacking Classifier\n",
        "estimators = [\n",
        "    ('rf', rf),\n",
        "    ('lr', lr)\n",
        "]\n",
        "# The final_estimator combines the predictions of the base estimators.\n",
        "stacking_clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(solver='liblinear'), cv=5)\n",
        "\n",
        "# 5. Train and Evaluate all models\n",
        "models = {\n",
        "    'Random Forest': rf,\n",
        "    'Logistic Regression': lr,\n",
        "    'Stacking Classifier': stacking_clf\n",
        "}\n",
        "\n",
        "accuracy_results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_results[name] = accuracy\n",
        "\n",
        "# Output the comparison\n",
        "print(\"--- Accuracy Comparison ---\")\n",
        "for name, accuracy in accuracy_results.items():\n",
        "    print(f\"{name}: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "Yn8G6F3Fjr0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a Bagging Regressor with different levels of bootstrap samples and compare performance"
      ],
      "metadata": {
        "id": "EBft5g2JjwVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "# 1. Generate Synthetic Data\n",
        "X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, noise=10, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 2. Define Estimator Levels\n",
        "n_estimators_list = [5, 10, 20, 50, 100, 200]\n",
        "results = []\n",
        "r2_scores = []\n",
        "\n",
        "# 3. Train and Evaluate\n",
        "for n in n_estimators_list:\n",
        "    bagging_reg = BaggingRegressor(\n",
        "        estimator=DecisionTreeRegressor(random_state=42),\n",
        "        n_estimators=n,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    bagging_reg.fit(X_train, y_train)\n",
        "    y_pred = bagging_reg.predict(X_test)\n",
        "\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "    results.append({\n",
        "        'n_estimators': n,\n",
        "        'R2 Score': r2,\n",
        "        'Mean Squared Error (MSE)': mse\n",
        "    })\n",
        "    r2_scores.append(r2)\n",
        "\n",
        "# 4. Plot R2 Score vs. n_estimators\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(n_estimators_list, r2_scores, marker='o', linestyle='-', color='indigo')\n",
        "plt.title('Bagging Regressor $R^2$ Score vs. Number of Estimators')\n",
        "plt.xlabel('Number of Bootstrap Samples ($n\\_{\\text{estimators}}$)')\n",
        "plt.ylabel('$R^2$ Score (Goodness of Fit)')\n",
        "plt.xticks(n_estimators_list)\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.savefig('bagging_regressor_performance_plot.png')"
      ],
      "metadata": {
        "id": "UFmoAGf-j0o1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}